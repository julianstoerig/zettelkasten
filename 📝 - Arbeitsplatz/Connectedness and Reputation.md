To support cooperation through a reputation mechanism requires that an individual’s neighbors know of a deviation. To assess the likelihood of word of a deviation spreading, we can apply three insights we learned when adding networks to the contagion model. First, the greater the degree of the network, the more likely it is that word of deviation would spread. Second, variation in the distribution of degrees, in particular the existence of superspreaders, would amplify the likelihood. Third, if an individual defects against someone who is not connected to any of the individual’s other neighbors, then the neighbors will not be likely to hear of the defection. Therefore, for reputations to spread, the network must have a high clustering coefficient. The clustering coefficient is a proxy for social capital. 

---

### Cooperation Among Rule-Playing Behaviors 

We now relax the assumption of rationality and assume that players apply rule-based strategies such as Grim Trigger. We will use our model to understand whether and how cooperation can emerge. Our model assumes a population of individuals who play repeated rounds of a Prisoners’ Dilemma game against one another. We assume that each interaction continues with some probability as above. That construction could induce rational players to cooperate if the probability of continuation is sufficiently high. Unlike above, here we assume that players apply behavioral rules. Some may play Grim Trigger. Others may always cooperate, and others may always defect. Variants of these strategies may be played by other species. Warbler males adopt a “dear enemy” strategy in which they do not engage in loud singing or fighting to extend their property at the expense of their neighbors. We can think of this as a cooperative action.^8 For ease of explanation, we assume that each individual plays with every other individual. After every individual has played all her games, each announces a performance equal to her average payoff in a play of the game. We use average per-game payoff rather than total payoff because some players may, by chance, play many more games than others given probabilistic continuation. In this model setup, a strategy’s performance depends on the distribution of strategies. It follows that the winning strategy can then also depend on the initial distribution. If cooperative strategies perform best initially, cooperation will likely grow in the population. For our analysis, we randomly assign to each player one of five behavioral rule strategies: always cooperate (All C), always defect (All D), Grim Trigger (GRIM), Tit for Tat (TFT), or TROLL. GRIM cooperates initially and continues cooperating until the opposing player defects, after which it defects forever. All C and All D do what their names imply: they blindly cooperate or defect regardless of the other player’s action. TFT cooperates in the first period and 

---

thereafter copies the action of the other player from the previous period; two players who both use TFT will always cooperate. TROLL seeks to exploit players who always cooperate. It defects in the first two periods, and if the other player does not defect in either of those periods, TROLL defects forever. If the other player does defect, TROLL switches to cooperate for two periods and thereafter plays GRIM. We first calculate the payoffs for each behavioral rule strategy playing against every other strategy using the payoffs from the Prisoners’ Dilemma in figure 22.1. We start with the strategy All D. If it plays against All C, it receives a payoff of 4 in every play of the game. All C, on the other hand, receives an average payoff of only 1 in those interactions. If All D plays against either TFT or GRIM, it receives a payoff of 4 in the first play and 2 thereafter. If we assume the game is repeated many times, this will average out to a little 

more than 2, so we write it as 2+. When All D plays TROLL, both defect in the first two periods, and TROLL cooperates in periods three and four but defects thereafter. All D again earns an average payoff of 2+. TROLL earns an average payoff of a little less than 2, 

which we write as 2−. We can perform similar exercises and compute the expected payoffs for every pair of strategies.^9 Table 22.1 shows the payoff for each strategy against each of the other strategies. 

 Table 22.1: Average Payoffs for Row Strategies Against Column Strategies 

The table reveals a mix of mutual cooperation, mutual defection, and strategies taking advantage of flaws in other strategies. A careful examination of the table reveals that four of the five 

---

strategies cooperate with themselves. We will refer to these as the potentially cooperative strategies. Only TFT cooperates with all four of these potentially cooperative strategies. So if any combination of those four accounted for the bulk of a population, TFT would perform 

well, if not best.^10 The thousands of human experiments run on the Prisoners’ Dilemma reveal tremendous heterogeneity in the strategies people choose. We will therefore use the payoffs in the table to think through the outcomes given different distributions. Based on the diversity of payoffs for the different combinations of strategies, the best strategy will depend on the composition of the population. In a population that consists mostly of All C, the strategy All D performs best. If individuals choose to adopt the best strategy, or if selection operates quickly, then the population might never manage to cooperate. If learning or selection happens at a moderate rate, players should move away from All C. Once the population contains few All C, All D will perform less well than GRIM, TROLL, and TFT. One of these strategies should take hold. This pattern of defectors performing well initially and then cooperation taking hold can be found in many experiments with human subjects as well as in simulations with computer-based artificial agents. We might describe what happens in those cases as the emergence or evolution of cooperation. One can imagine any distribution across these five strategies or any other ensemble of strategies, compute average payoffs, and then think through what might occur through learning or selection. In a later chapter, we construct formal models of learning and selection. We rely here on informal arguments, as we only wish to make the point that whether cooperation emerges depends on the initial strategies in population and how people learn or evolve new strategies. A necessary condition for cooperation to emerge or evolve is that the payoff from cooperating exceeds the payoff from defecting given the population. Otherwise, both selection and learning would lead the population toward defection. To simplify the analysis, we can 

---

imagine a population that consists of cooperative strategies, such as GRIM, All C, and TFT, and defecting strategies, such as All D. We can then calculate what would have to be true for the cooperative strategies to perform better on average. That calculation reveals that evolving cooperation is more difficult than maintaining cooperation, and that cooperation cannot bootstrap itself—a small population of 

cooperators cannot cause cooperation to emerge.^11 This distinction between maintaining coordination, emerging or evolving coordination, and bootstrapping coordination merits revisiting. Cooperation can be maintained if, when all players cooperate, cooperation performs best. Maintenance corresponds to cooperation through GRIM being a Nash equilibrium of the repeated game. Cooperation can emerge or evolve if the strategies that cooperate when paired in a population outperform, on average, those that do not. As just argued, the conditions for emergence of cooperation are harder to satisfy than the conditions for maintenance of cooperation. In fact, the mathematics shows us that bootstrapping is all but impossible. If the proportion of cooperators is near zero, then cooperators earn lower payoffs than defectors. The takeaway should not be that bootstrapping coordination can never occur, only that it cannot happen in this model. To obtain cooperation, we need a proportion of people to cooperate initially. That could happen with people who reflect on the game, but it seems less likely for bees and tree roots. To understand how bootstrapping could occur, we need more elaborate models that allow for local learning, evolution, and group selection. We turn to those now. 

---

### Cooperative Action Model 

To study how cooperation can emerge, we introduce a _cooperative action model_ in which individuals can either take a cooperative action or refrain from doing so.^12 The cooperative action imposes a cost on the individual and produces a benefit to others. Refraining from action imposes no cost and produces no benefit. There are several differences between the cooperation action model and the repeated Prisoners’ Dilemma. First, the individuals in the cooperative action model are not playing a repeated pairwise game in which they apply strategies and earn payoffs. Instead, individuals are either cooperators or non-cooperators. Second, the model does not assume rational actors or individuals who apply more sophisticated rules. Third, the individuals belong to an interaction network. Their cooperative actions impact only those with whom they are connected, their neighbors. Last, because the individuals have fixed types, they take the same action with all of their neighbors. A cooperating individual with five neighbors pays the cost of cooperating five times and produces a benefit to five others. 

---