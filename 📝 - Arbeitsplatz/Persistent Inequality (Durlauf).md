Schelling Segregation Model + Local Majority Model Individuals belong to income classes and segregate residentially by income. Individuals allocate a portion of their income to education, resulting in positive spillovers that increase with community income level. The future income of a child living in community C depends on her innate ability, spending on education, and spillovers. The contributions of education and spillovers depend on the level of income within the community, IC. 

 Income C = F(ability, education( IC ), spillover( IC )) 

 Cause of inequality: Children who grow up in low-income neighborhoods receive fewer educational opportunities and economic spillovers. 

In the complete model, Durlauf solves for equilibrium levels of educational spending and derives conditions in which persistent inequality arises. That inequality results from what he calls _poverty traps_. Individuals living in low-income communities lack the educational resources and levels of spillovers necessary to earn high incomes regardless of their ability levels. Durlauf’s model can help to explain the enormous racial gaps in income levels. African Americans disproportionately live in poor neighborhoods, and as a result, they may become trapped in low income trajectories because of a lack of resources. These various models describe distinct causes of income inequality. In a sense, each is correct, but, as we know, each model is also wrong. Empirically, the models differ in how much and what part of the variation in income they can explain. For the upper end of the income distribution, the empirical evidence most strongly 

---

supports the models that rely on technological change.^16 For over twenty years, the IRS has tracked the highest 400 incomes. Those at the top of the distribution come from technology, mass retail, and finance, three industries that can scale quickly. That high growth rate could stem from winner-take-all markets for search engines (Google) or social networking sites (Facebook). These models tell us little about the lower end of the income distribution. Nor do they say much about income mobility, or explain why CEO pay in the United States far exceeds that in other countries. To explain these other features of the data, we need the other models, such as the income mobility model, Durlauf’s persistent inequality model, and the spatial voting model. By constructing a dialogue between multiple models and data, we come away with a deep, multifaceted understanding of the causes of inequality. We identify multiple processes that produce and maintain inequality and see how they overlap and intersect. Our understanding of the complexity of inequality and the self-reinforcing causal forces that sustain it should make us dubious of quick fixes. Reducing inequality will require concentrated efforts on multiple fronts. 

---

### Into the World 

We have just learned how by applying many models as an ensemble we can explicate the multiple causes of the opioid epidemic and income equality and reveal the limits of any one framing. Were we policymakers, we could fit some of these models to data to gauge effect sizes. We could then run natural experiments to help us guide policy choices based on what we have learned. We could also take the many model approach to any number of social challenge including reversing trends in obesity, improving school performance, mitigating climate change, managing water resources, and improving international relations. In each case, even adding a single new model could have enormous consequences. Take for example the problem of predicting financial collapses. The United States Federal Reserve relies on traditional economic models using national accounting data on inflation, unemployment, and inventories. Those data suffer from lags. They are released weekly, quarterly, or annually. Those data also come from surveys, that is, samples of the entire economy. Complexity scholar, J. Doyne Farmer argues for creating a second class of models based on real time data scraped from the web. These new models would rely on more granular, instantaneous data, and therefore differ from traditional models. Farmer argues that such models could prove much better than existing models. He may be right. Yet, these new models need not be more accurate to be of use in predicting and preventing financial disasters. Given the new models would use different data and rely on different assumptions, they would make different predictions. As we know from the diversity prediction theorem so long as the new models are not far less accurate, when combined with existing models, these new models would improve predictions. Policy makers, to use Farmer’s turn of phrase, would be more collectively aware.^17 An executive might engage in a similar exercise when making a business decision. She could apply multiple models informed by data 

---

to decide on product attributes, time product launches, design compensation plans, construct supply chains, and forecast sales. Because each of these actions occurs within a complex system, any one model would be wrong. Many models would lead to better actions. To sum up, when confronted with a choice, when asked to make a prediction, or when faced with a design challenge, we should take a many-model approach. Many-model thinking produces better performance than taking actions based on hunches and gut instincts. That said, we have no guarantee of success. Even with many models, we may not identify the most relevant logical chain. The domain of interest might be so complex that even ensembles of models can only explain a small portion of the variation. The same holds when applying models to aid in design, we may find ourselves unable to construct useful abstractions. The simplicity of models may, in those cases, be their undoing. In the face of complexity, it is possible that we find models not up to the tasks of communicating ideas, making accurate predictions, or pointing us toward the best actions. Even our explorations with models may reveal little of value. In those instances, the REDCAPEs that the book has promised will not provide much lift. Nevertheless, even in those cases, we benefit from contemplating and applying many models. In doing so, we uncover interdependencies. We learn why a complex process can frustrate our attempts to understand, explain, or communicate. Thus, we must maintain a degree of humility. Even aided by many models, our abilities to reason have limits. For that reason, we must remain curious. We must continue to build new models and to improve upon existing ones. If a model leaves out key features of the world—such as social influences, positive feedbacks, or cognitive biases—then we should build other models that include those features. By doing so, we can begin to discern when those attributes matter and how much. The fact that all models are wrong should not take the wind out of our sails but be seen as motivation for building a crowd of models capable of producing wisdom. Last, we should seek joy in those efforts. Though the book has 

---

emphasized pragmatic aims—to become better thinkers, to be more effective at work, and to operate as more informed citizens of the world—it has also had an implicit goal of revealing the beauty of models and the fun of modeling. The practice of modeling can be a beautiful game. We make the assumptions, write the rules, and then play within those rules bound also by the laws of logic. Through our logical explorations, we improve ourselves and become wise. May we take that wisdom out into the world and help to change it in positive ways. 

---

**SCOTT E. PAGE** is the Leonid Hurwicz Collegiate Professor of Complex Systems, Political Science, and Economics at the University of Michigan and an external faculty member of the Santa Fe Institute. 

_Photo Credit: Cooper Page_ 

---