A collection of alternatives { A, B, C, D,..., N } have associated rewards { π ( A ), π ( B ), π ( C ), π ( D ),..., π ( N )} and a set of strictly positive weights { w ( A ), w ( B ), w ( C ), w ( D ),..., w ( N )}. The probability of choosing K is as follows: 

 After choosing K , w ( K ) increases by γ · P ( K ) · ( π ( K ) − A ), where γ > 0 equals the rate of adjustment and < max Kπ ( K ) equals the aspiration level.^5 

Notice that the aspiration level must be set below the reward for at least one alternative. Otherwise, any alternative chosen becomes less likely to be chosen in the future and all of the weights converge to zero. It can be shown that if the aspiration level is below the reward for at least one alternative, eventually almost all of the weight will be placed on the best alternative. This occurs because each time the best alternative is selected, its weight increases by the most, creating stronger reinforcement of that alternative. This occurs even if we set the aspiration level below the reward from each alternative. In that case, every alternative increases in weight when selected. Thus, the model can capture _habituation,_ where we do more of something just because we have done it in the past. Even with a low aspiration level, the alternatives with the highest rewards increase in weight the fastest, so the best alternative wins out in the long run. However, the time required for convergence on the best alternative may be long. It will also be true that as we add more alternatives, time to convergence also increases. To avoid these complications, we can build in _endogenous aspirations_. We emend the model so that the aspiration level adjusts over time by setting it equal to the average reward. Imagine a parent 

---

learning whether a child prefers apple pancakes or banana pancakes. Assign a reward of 20 to apple pancakes and 10 to banana pancakes. Set the initial weights on both alternatives to 50, the rate of adjustment to 1, and the aspiration level to 5. Assume the parent makes banana pancakes the first day. The weight on banana pancakes will increase to 55. Suppose that parent makes banana pancakes the next day as well. The reward of 10 equals the new aspiration level, so the weight on banana pancakes does not change. Suppose that on the third day the parent makes apple pancakes. These produce a reward of 20, 10 above the aspiration level. This increases the weight on apple pancakes to 60, making them the more likely choice. The high reward also increases the average payoff, and therefore the aspiration level, above 10. Thus, if the parent makes banana pancakes again, the weight on banana pancakes decreases because the reward from banana pancakes lies below the new aspiration level. Reinforcement learning therefore converges to only apple pancakes being selected. It can be proven that reinforcement learning will converge toward selecting the best alternative with probability 1. That means that the weight on the best alternative will become arbitrarily large compared to the weights on all other alternatives. 

---