where V ( x ) equals the value of x in X , equals the average value, and M ( x ) equals the model’s valuation. 

In this context, a categorization model would partition the households into categories and estimate a value for each category. A more granular model would create more categories. This may require considering more attributes of the households to create those categories. As we add more categories, we can explain more of the variation, but we can go too far. If we follow the example of Borges’s mapmakers and place each household in its own category, we can explain all of the variation. That explanation, like the life-sized map, would not be of much use. Creating too many categories overfits the data, overfitting undermines prediction of future events. Suppose that we want to use last month’s data on grocery purchases to predict this month’s data. Households vary in their monthly spending. A model that places each household in its own category would predict that each household spends the same as in the previous month. That would not be a good predictor given monthly fluctuations in spending. By placing the household into a category with other similar households, we can use the average spending on groceries for similar households to create a more accurate predictor. To do this, we think of each household’s monthly purchases as a draw from a distribution (we will cover distributions in Chapter 5 ). That distribution has a mean and a variance. The objective in creating a categorization model is to construct categories based on attributes so that the households within the same category have similar means. If we can do that, one household’s spending in the 

---

first month tells us about the other households’ spending in the second month. No categorization will be perfect. The means of households within each category will differ by a little. We call this _categorization error_. As we make larger categories, we increase categorization error, as we are more likely to clump households with different means into the same category. However, these larger categories rely on more data, so our estimates of the means in each category will be more accurate (see the square root rules in Chapter 5 ). The error from misestimating the mean is called the _valuation error_. Valuation error decreases as we make categories larger. One or even ten houses per category will not give an accurate estimate of the mean if households vary substantially in their monthly spending. A thousand households will. We now have the key intuition: increasing the number of categories decreases the categorization error from binning households with different means into the same category. Statisticians call this _model bias_. However, making more categories increases the error from estimating the mean within each category. Statisticians refer to this as increasing the _variance_ of the mean. The trade-off in how many categories to create can be expressed formally in the _model error decomposition theorem_. Statisticians refer to the result as the bias-variance trade-off. 

---